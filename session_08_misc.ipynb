{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this session we'll make use of the pathlib library. It provides an object-oriented approach to represent filesystem paths and works across operating systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom problems\n",
    "\n",
    "## Calculating ETo\n",
    "\n",
    "The code below provides some ideas for implementing an evapotranspiration module as part of a water balance model. The real version of this code should use the Penman-Monteith equation, but to simplify the code example, the Hargreaves equation is demonstrated here instead.\n",
    "\n",
    "The code could be organised in a class (called `ETModel`) that loads the required data, calculates ETo and creates a multi-graph figure of the data and the calculated ETo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETModel:\n",
    "    def __init__(self, workspace=\"et_model\"):\n",
    "        self.df = None\n",
    "        self.workspace = workspace\n",
    "\n",
    "    def load_data(self, xl_fname):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        xl_fname : str\n",
    "            Name of the Excel file with the input data.\n",
    "        workspace : str\n",
    "            Name of the folder with the model data. Also used for output.\n",
    "        \"\"\"        \n",
    "        try:\n",
    "            fpath = Path(self.workspace, xl_fname)\n",
    "            self.df = pd.read_excel(\n",
    "                fpath,\n",
    "                sheet_name=\"Climate input template\",\n",
    "                skiprows=3,\n",
    "                index_col=0,\n",
    "                parse_dates=True,\n",
    "            )\n",
    "        except:\n",
    "            print(\"Something went wrong...\")\n",
    "\n",
    "    def ET_Hargreaves(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        This method calculates the reference ET using the Hargreaves equation. See:\n",
    "        https://pyeto.readthedocs.io/en/latest/_modules/pyeto/fao.html#hargreaves\n",
    "        \"\"\"\n",
    "        try:\n",
    "            tmin = self.df[\"Tmin\"]\n",
    "            tmax = self.df[\"Tmax\"]\n",
    "            tmean = (tmin + tmax) / 2.\n",
    "            et_rad = self.df[\"Radn\"]\n",
    "\n",
    "            self.df[\"ETo\"] = 0.0023 * (tmean + 17.8) * (tmax - tmin) ** 0.5 * 4.08e-4 * et_rad\n",
    "        except:\n",
    "            print(\"Something went wrong...\")\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        This method plots the input and output data.\n",
    "        \"\"\"        \n",
    "        fig, axs = plt.subplots(nrows=3, figsize=(10, 8))\n",
    "        ax = axs[0]\n",
    "        ax.plot(self.df.index, self.df[\"Tmax\"], label=\"Max temp (C)\")\n",
    "        ax.plot(self.df.index, self.df[\"Tmin\"], label=\"Min temp (C)\")\n",
    "        ax.set_ylabel(\"Temperature (\\u00B0C)\")\n",
    "\n",
    "        ax = axs[1]\n",
    "        ax.bar(self.df.index, self.df[\"Rain\"], width=self.df[\"Durn\"])\n",
    "        ax.set_ylabel(\"Daily rainfall (m)\")\n",
    "\n",
    "        ax = axs[2]\n",
    "        ax.bar(self.df.index, self.df[\"ETo\"] / 1000, width=1)\n",
    "        ax.set_ylabel(\"ETo (m)\")\n",
    "        ax.set_xlabel(\"Year\")\n",
    "\n",
    "    def save_results(self, fname=\"et_results.csv\"):\n",
    "        \"\"\"\n",
    "        This method saves the DataFrame with the calculated evaporation rates\n",
    "        \"\"\"        \n",
    "        Path(self.workspace, \"results\").mkdir(exist_ok=True)\n",
    "        fpath = Path(self.workspace, \"results\", fname)\n",
    "        self.df.to_csv(fpath)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The various class methods can be called by initializing an instance of `ETModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_model = ETModel()\n",
    "et_model.load_data(\"Example dataset.xlsx\")\n",
    "et_model.ET_Hargreaves()\n",
    "et_model.plot()\n",
    "et_model.save_results()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling model parameters from an Excel workbook\n",
    "\n",
    "Various  model parameters can be extracted from the Excel file in the following way, for example, the soil albedo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = Path(\"et_model\", \"Example dataset.xlsx\")\n",
    "dfv = pd.read_excel(\n",
    "    fpath,\n",
    "    sheet_name=\"Veg data input\",\n",
    "    skiprows=1,\n",
    "    header=None,\n",
    "    names=[\"Parameter\", \"Pasture\", \"Wetland\"],\n",
    "    usecols=\"E,F,N\",\n",
    "    index_col=0,\n",
    ")\n",
    "\n",
    "for col in dfv.columns:\n",
    "    row_name = \"One minus Albedo of Soil\"\n",
    "    soil_albedo = 1 - dfv.loc[row_name, col]\n",
    "    print(f\"The soil_albedo of {col} is {soil_albedo}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to combine the data from different work sheets. The following code example reads the soil parameters in the worksheet called 'BW table (soil generation)'. It then uses the numerical code for the soil type in the soil profile information (which is in the worksheet 'Node table (soil)') to assign the saturated hydraulic coductivity to each node of the soil profile model. The result is plotted using the Matplotlib function `step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.read_excel(\n",
    "    fpath,\n",
    "    sheet_name=\"BW table (soil generation)\",\n",
    "    skiprows=[r for r in range(14) if r not in [2, 3, 7, 11]],\n",
    "    usecols=\"C:G\",\n",
    ")\n",
    "\n",
    "dfn = pd.read_excel(\n",
    "    fpath,\n",
    "    sheet_name=\"Node table (soil)\",\n",
    "    skiprows=1,\n",
    "    usecols=\"C:G\",\n",
    ")\n",
    "\n",
    "col_name = \"Ks\"\n",
    "dfn[col_name] = np.nan\n",
    "for i, row in dfs.iterrows():\n",
    "    idx = dfn[\"Soil type indicator\"] - 1 == i\n",
    "    dfn.loc[idx, col_name] = row[col_name]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(3, 4))\n",
    "ax.step(dfn[col_name], dfn[\"Depth(mBGL)\"])\n",
    "ax.set_xlabel(\"Saturated conductivity (m/day)\")\n",
    "ax.set_ylabel(\"Depth (mBGL)\")\n",
    "ax.set_ylim(dfn[\"Depth(mBGL)\"].iloc[-1], dfn[\"Depth(mBGL)\"].iloc[0]);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise***: Expand the previouse example by adding a second graph that shows the residual water content ('thetar') as a function of depth."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting a number of csv files to a different format\n",
    "\n",
    "The code below shows how to iterate over a number of comma-separated values files in a folder and convert each to a different format. First, let's create a list with all csv files in the folder csv_files. We use the `glob` method of the `Path` object to list all the csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_fnames = Path('csv_files').glob('*.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can loop over the file names in the list, read their contents and save them in the desired format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Path to create subfolder 'txt_files' if it does not exist yet\n",
    "Path('txt_files').mkdir(exist_ok=True)\n",
    "\n",
    "for input_fname in csv_fnames:\n",
    "    df = pd.read_csv(\n",
    "      input_fname, \n",
    "      index_col=0,\n",
    "      parse_dates=True,\n",
    "    )\n",
    "\n",
    "    output_fname = Path(str(input_fname).replace(\"csv\", \"txt\"))\n",
    "\n",
    "    df.to_csv(\n",
    "        output_fname, \n",
    "        date_format=\"%d/%m/%Y\", \n",
    "        sep='\\t',\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data now appear in the file in the desired format. But the goal is to also create a preamble with three lines containing some additional information. This is possible by opening the txt files in 'append' mode, then writing the preamble lines first, and then use the `to_csv` method to write the acutal data. Note that the file must be closed at the end. Because the file is openend in append mode, the original txt files must be deleted first (otherwise they'd get longer and longer upon repeated use of the code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second attempt, with preamble\n",
    "csv_fnames = Path('csv_files').glob('*.csv')\n",
    "\n",
    "# Delete any existing txt files\n",
    "txt_fnames = Path('txt_files').glob('*.txt')\n",
    "for txt_fname in txt_fnames:\n",
    "    txt_fname.unlink()\n",
    "\n",
    "for input_fname in csv_fnames:\n",
    "    \n",
    "    # Read the original csv file\n",
    "    df = pd.read_csv(\n",
    "      input_fname, \n",
    "      index_col=0,\n",
    "      parse_dates=True,\n",
    "    )\n",
    "\n",
    "    # Open output file as text file\n",
    "    output_fname = Path(str(input_fname).replace(\"csv\", \"txt\"))\n",
    "    f = open(output_fname, 'a')\n",
    "    \n",
    "    # Write preamble\n",
    "    \n",
    "    f.write('012345 Station name\\n')\n",
    "    f.write('Calculated value \\n')\n",
    "    f.write('m3/hr\\n')\n",
    "    \n",
    "    # Export DataFrame\n",
    "    df.to_csv(\n",
    "        f, \n",
    "        date_format=\"%d/%m/%Y\", \n",
    "        sep='\\t',\n",
    "        header=False,\n",
    "        lineterminator='\\n',\n",
    "    )\n",
    "    \n",
    "    # Close the file\n",
    "    f.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading data from a REST API\n",
    "\n",
    "In session 4, Onno demonstrated <A href=\"https://github.com/ArtesiaWater/hydropandas\">Hydropandas</A>. It pulls online data into a Pandas DataFrame. A similar library was developed by Kent Inverarity for the groundwater data in the South Australian WaterConnect database, see\n",
    "<A href=\"https://github.com/kinverarity1/python-sa-gwdata\">https://github.com/kinverarity1/python-sa-gwdata</A>. The key command behind both libraries is the `get` method of the `requests` package, which will attempt to retrieve data from a specified source. The request is made by passing an url with a specifc stucture, which is defined by the application programming interface (API) of the service that is being queried. Several protocols exists, with the most common one today being the representational state transfer architectural style (REST), and an API that conforms to this standard is called a RESTful API.\n",
    "\n",
    "Without providing any more technical details, let's just try to see how this works. Click the following link and observe the information that appears in your web browser.\n",
    "\n",
    "<A href=\"https://www.waterconnect.sa.gov.au/_layouts/15/dfw.sharepoint.wdd/WDDDMS.ashx/GetObswellNumberSearchData?OBSNUMBER=WLG051\">https://www.waterconnect.sa.gov.au/_layouts/15/dfw.sharepoint.wdd/WDDDMS.ashx/GetObswellNumberSearchData?OBSNUMBER=WLG051</A>\n",
    "\n",
    "The data obtained appear in the form of a table and it can be seen that there are several fields. This is not very useful yet, but it gives you an idea of the information that is sent when you use the `get` method. If you look at the above url, it is possible to recognise three parts:\n",
    " * a base url: https://www.waterconnect.sa.gov.au/_layouts/15/dfw.sharepoint.wdd/WDDDMS.ashx/\n",
    " * a command: 'GetObswellNumberSearchData'\n",
    " * a section with parameters for the command: 'OBSNUMBER=WLG051'\n",
    "\n",
    "From this we can infer that this url requests to search data based on the Obswel number, which in this case is specified to be WLG051 (not entirely coincidentally, this is the monitoring bore right next to the farm dam).\n",
    "\n",
    "Now let's do this in Python using the `get` method. We create a variable `url`, which combines the base url and the command and we pass the command parameters as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "base_url = \"https://www.waterconnect.sa.gov.au/_layouts/15/dfw.sharepoint.wdd/WDDDMS.ashx/\"\n",
    "url = base_url + \"GetObswellNumberSearchData\"\n",
    "rest_params = {\"OBSNUMBER\": \"WLG051\"}\n",
    "\n",
    "response = requests.get(url, params=rest_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned data are stored in `response`. This is an object that contains information about the request and, if the request was successful, the data in JSON format (note that this differs depending on the API, other services may use a different format, e.g. csv). JSON is shorthand for JavaScript Object Notation and is a common data-interchange format. Although it is intended to be readable for humans, it is not as convenient as a DataFrame, so the next code cell converts the data to a DataFrame called `df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = response.json()\n",
    "df = pd.json_normalize(response.json())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the DataFrame in the variable explorer shows that it has a column 'DHNO', which stands for drillhole number. We can use this number in combination with the API command 'GetWaterLevelDetails' to get the water level time series for this well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dhno = df[\"DHNO\"]\n",
    "url = base_url + \"GetWaterLevelDetails\"\n",
    "rest_params = {\"DHNO\": dhno}\n",
    "\n",
    "response = requests.get(url, params=rest_params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we can convert the JSON data to a DataFrame. The data in the column 'OBS_DATE' can be converted to a datetime format, and be used as the index of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = response.json()\n",
    "df = pd.json_normalize(response.json())\n",
    "df.index = pd.to_datetime(df[\"OBS_DATE\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the data is then a breeze of course..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(df.index, df[\"RSWL\"], '.-')\n",
    "ax.set_title(\"WLG051\")\n",
    "ax.set_ylabel(\"Year\")\n",
    "ax.set_ylabel(\"RSWL\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples above demonstrate the use of the `get` method. Libraries like <A href=\"https://github.com/kinverarity1/python-sa-gwdata\">python-sa-gwdata</A> and <A href=\"https://github.com/ArtesiaWater/hydropandas\">Hydropandas</A> wrap Python code around this method to provide a user-friendly way to obtain data from a database. If you work with chemicals then <A href=\"https://pubchempy.readthedocs.io/en/latest/\">PubChemPy</A> is another interesting package to look at, as it allows you to access the data in the <A href=\"https://pubchem.ncbi.nlm.nih.gov/\">PubChem</A> database. More RESTful APIs exist, and their number is growing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyQGIS \n",
    "QGIS (and ArcGIS) offers Python support via the QGIS Python API. Extensive documentation is provided <A href=\"https://docs.qgis.org/3.28/en/docs/pyqgis_developer_cookbook/index.html#\">here</A>. The example below will use GeoPandas to create a shapefile of some data downloaded from  WaterConnect. The shapefile will be imported into QGIS and PyQGIS will be used to create three separate layers showing the boreholes with chemistry, water (level) and salinity data, respectively. The PyQGIS interface will also be used to set a different marker symbol colour for each of the three layers.\n",
    "\n",
    "Let's start by using the WaterConnect API to download the availabe boreholes in a rectangular area near McLarenVale, SA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = base_url + \"GetGridData\"\n",
    "rest_params = {\"Box\": \"-35.25,138.55,-35.20,138.6\"}\n",
    "response = requests.get(url, params=rest_params)\n",
    "data = response.json()\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.json_normalize(response.json())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Geopandas, the DataFrame can be converted to a shapefile with just a few lines of code. First the data in the columns 'LON' and 'LAT' will be used to create the coordinate data. Together with the DataFrame `df` the coordinate data in `lat_long_coordinates` is used to create a GeoDataFrame (note that the EPSG code 4326 is for lat/long coordinates based on the World Geodetic System 1984 ensemble (WSG84)). The method `to_file` saves the shapefile to disk, and the `mkdir` call ensures that the subdirectory 'shp' exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "lat_long_coordinates = gpd.points_from_xy(df[\"LON\"], df[\"LAT\"])\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df, \n",
    "    geometry=lat_long_coordinates, \n",
    "    crs=\"EPSG:4326\",\n",
    ")\n",
    "\n",
    "Path('shp').mkdir(exist_ok=True)\n",
    "gdf.to_file(\"shp/borehole_data.shp\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will only work inside the QGIS Python editor (as will be demonstrated during the session). Note that the information on the object model for QGIS is extensive. For example, the documentation for a map layer object can be found <A href=\"https://api.qgis.org/api/classQgsMapLayer.html\">here</A> and there are many, many more (see <A href=\"https://api.qgis.org/api/modules.html\">https://api.qgis.org/api/modules.html</A>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_project = QgsProject.instance()\n",
    "\n",
    "layer = current_project.mapLayersByName('borehole_data')[0]\n",
    "\n",
    "field_names = [\"CHEM\", \"WATER\", \"SAL\"]\n",
    "\n",
    "for field_name in field_names:\n",
    "    new_layer = layer.clone()\n",
    "    new_layer.setName(f'{field_name}_data')\n",
    "    new_layer.setSubsetString(f'\"{field_name}\" = \\'Y\\'')\n",
    "    current_project.addMapLayer(new_layer)\n",
    "\n",
    "colors = [\"red\", \"green\", \"blue\"]    \n",
    "for field_name, color in zip(field_names, colors):\n",
    "    layer = current_project.mapLayersByName(f'{field_name}_data')[0]\n",
    "    layerRenderer= layer.renderer()\n",
    "    mSingleRenderer = QgsSingleSymbolRenderer.convertFromRenderer(layerRenderer)\n",
    "\n",
    "    new_sym = QgsMarkerSymbol.createSimple({\"color\": color})\n",
    "    mSingleRenderer.setSymbol(new_sym)\n",
    "    layer.setRenderer(mSingleRenderer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geopandas_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
