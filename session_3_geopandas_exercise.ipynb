{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with geospatial data\n",
    "\n",
    "In this session we will practice working with geospatial data using geopandas. The demonstration dataset consists of a bathymetry point measurements and the objective is to interpolate the depth of the farm dam. It will be demonstrated how this can be done easily using different interpolation methods. The interpolated data will then be used to derive a relationship between water volume and water level, which serves as input for the water balance model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is import the packages needed for this exercise. In the code cell below, observe how we only import the function `griddata` from `scipy.interpolate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgeopandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mgpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'geopandas'"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.interpolate import griddata\n",
    "import shapely.geometry as shg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have to use `set_data_dir` if you get an error that the path to the file `proj.db` can't be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj.datadir import set_data_dir\n",
    "\n",
    " set_data_dir(\"c:/Users/PRI258/anaconda3/envs/geopandas_env/Library/share/proj/\")\n",
    "# set_data_dir(\"c:/Users/VincentPost/anaconda3/envs/geopandas_env/Library/share/proj/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are in an Excel file in the 'data' folder with the name 'dam_bathymetry.xlsx'. It can be easily imported using the Pandas `read_excel` function. Note that the column names are specified explicitly, rather than imported from the Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\n",
    "    \"data/dam_bathymetry.xlsx\",\n",
    "    skiprows=5,\n",
    "    usecols=\"B,E,F\",\n",
    "    header=None,\n",
    "    names=[\"z\", \"easting\", \"northing\"],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one point in the land survey data that is not a depth of the dam, but the reference point height, so it should not be included in interpolation. It can be found by selecting the row with value zero in the column 'z' of the DataFrame. It is subsequently removed by using the `.loc` slicing option for a DataFrame (note how the `~` sign negates the elements of `idx` so all the rows where z is not zero will be retained in `df`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = df['z'] == 0\n",
    "df = df.loc[~idx]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to save this data to a shapefile so that we can have a look at the data in QGIS. First we have to store the coordinates of the points in a list. The list consists of objects of the `Point` class from `shapely.geometry`, which takes a tuple with the coordinate pair as an initialization argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_list = []\n",
    "for index, row in df.iterrows():\n",
    "    pt = shg.Point((row[\"easting\"], row[\"northing\"]))\n",
    "    pt_list.append(pt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the list of points we can create a GeoDataFrame, which is the main strength of the geopandas library: This data structure has all the properties and methods of a Pandas DataFrame, but it couples the data to geospatial information. For each row in the GeoDataFrame there is a spatial object (e.g. a `Point` or a `Polygon`), which is stored in the column 'geometry'. The definition of the GeoDataFrame with the variable name `gdf` is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(\n",
    "    data=df[\"z\"], \n",
    "    geometry=pt_list, \n",
    "    crs=\"epsg:32754\",\n",
    ")\n",
    "\n",
    "gdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `to_file` function we can save it as a shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir(\"data/dam_bathymetry/\")\n",
    "gdf.to_file(\"data/dam_bathymetry/dam_bathymetry.shp\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import the polygon with z = -20 cm (hand-drawn in QGIS to assist in the interpolation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_h = gpd.read_file(\"data/helper_poly/helper_poly.shp\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the interpolation we need the coordinates of the vertices that we created in QGIS,  which we can pull out of the imported GeoDataFrame `gdf_h`. There is ony one `Polygon` feature, so in the first line of code we select the first element of the 'geometry' objects in `gdf_h`. The second line of code obtains the coordinate pairs from the `Polygon`'s `exterior` property, which in turn has a property `xy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_poly = gdf_h['geometry'][0]\n",
    "poly_vertices = helper_poly.exterior.xy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then store easting, northing and the z values in a DataFrame that has the same columns and column names as `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfh = pd.DataFrame()\n",
    "dfh['easting'] = poly_vertices[0]\n",
    "dfh['northing'] = poly_vertices[1]\n",
    "dfh['z'] = -20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now merge the two DataFrames `df` and `dfh` to get a single DataFrame that we'll use for the interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat((df, dfh))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though it is redundant to store the same information in different variables, for convenience we'll store the coordinate and depth data in separate arrays `x`, `y` and `z`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['easting'].to_numpy()\n",
    "y = df['northing'].to_numpy()\n",
    "z = df['z'].to_numpy() / 100. # From cm to m"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the interpolation we'll need to define a regular grid which uses the same coordinates as the data points. The values for `X_EASTING` and `Y_NORTHING` as well as the extents were found by trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_EASTING = 279967.34\n",
    "Y_NORTHING = 6098781.6\n",
    "dx = 0.5\n",
    "dy = dx\n",
    "xi = np.arange(-35., 40.5, dx) + X_EASTING\n",
    "yi = np.arange(-35, 15.5, dy)  + Y_NORTHING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NumPy function `meshgrid` creates a set of coordinate pairs, which will be used to interpolate the irregular data points to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = np.meshgrid(xi, yi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check if our regular grid overlaps the entire dam, we can quickly plot the grid points (in black) and the original data points (in red)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, Y, 'k.')\n",
    "plt.plot(x, y, 'r.');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code cell contains a `for` loop that steps over the three available interpolation methods for the `griddata` function. It creates two figures with each three subplots that show the results of the interpolation, which we'll use for visual inspection of the interpolation results. Note the use of the `add_subplot` function to create the 3d plot. The 'helper points' that we added manually in QGIS are plotted separately as red dots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_levels = np.arange(-2.75, 0, 0.25)\n",
    "fig3d = plt.figure()\n",
    "fig_contours, axs_contours = plt.subplots(ncols=3)\n",
    "for i, method in enumerate([\"nearest\", \"cubic\", \"linear\"]):\n",
    "    zi = griddata((x, y), z, (xi[None, :], yi[:, None]), method=method)\n",
    "\n",
    "    ax = axs_contours[i]\n",
    "    cs = ax.contourf(X, Y, zi, cl_levels)\n",
    "\n",
    "    ax = fig3d.add_subplot(1, 3, i+1, projection='3d')\n",
    "    ax.plot_wireframe(X, Y, zi, rstride=4, cstride=4)\n",
    "    ax.scatter(x, y, z, color='k')\n",
    "    \n",
    "    # Helper points\n",
    "    ax.scatter(x[z==-0.20], y[z==-0.20], z[z==-0.20], color='r')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is actually possible to export the contour lines that Matplotlib has drawn to a shapefile. We have to dive deep into the Matplotlib documentation for this, but it can be done. The code to do this is rather complicated. The first line of the next code cell gets the levels and the lines themselves from the variable `cs` and creates a dictionary. By looping through the items of the dictionary, we can create two lists: One containing the levels and one containing the polygons that we wish to save to the shapefile. Under the hood, the filled contours visible in the figure are instances of Matplotlib's `Path` class. For each contour interval, there's one or more `Path` instances, which can be grabbed with the `get_paths` method. This returns a list with all the `Path` instances for a given contour level. In most cases there's only one, so we can select the last element from the list of `Path` objects returned by `get_paths`, which is why there is `xys[0]`. The `to_polygons` method of the `Path` class returns a set of x and y values that we can convert to a Shapely `Polygon` object, which is the object that is added to the `poly_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvl_lookup = dict(zip(cs.levels, cs.collections))\n",
    "\n",
    "level_list = []\n",
    "poly_list = []\n",
    "for k, v in lvl_lookup.items():\n",
    "    level_list.append(k)\n",
    "    xys = v.get_paths()\n",
    "    xy = xys[0].to_polygons()[0]\n",
    "    poly_list.append(shg.Polygon(xy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two lists can be converted to a GeoDataFrame. Note that the dictionary passed as the `data` argument creates a column with the level values, and uses the items in `level_list` as values. The `poly_list` is used for the geometry objects. Once the file is saved it can be inspected in QGIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.mkdir(\"data/interpolated_contours\")\n",
    "gdf = gpd.GeoDataFrame(data={'level': level_list}, geometry=poly_list, crs=\"epsg:32754\")\n",
    "gdf.to_file(f\"data/interpolated_contours/interpolated_contours_{method}.shp\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The next step is  to create an array with depths for which we will calculate the surface area and volume of the water in the dam at those levels. The deepest depth is determined by rounding the minimal `zi` value to the nearest centimeter (hence the `* 100`), and then converting the result back to meter (using the `/ 100.`). The use of `arange` ensures that the surface area and volume will be calculated for each centimeter between `z = -0.2` m and `z_min`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_min = int(np.nanmin(zi) * 100) / 100\n",
    "wls = np.arange(-0.2, z_min, -0.01)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `for` loop below steps over each water level in `wls`. The water depth `wd` is equal to `wl` minus the bottom elevation `zi`. If the water level `wl` is greater than the bottom elevation `zi`, the water depth `wd` is positive. To determine the surface area, we can count the number of cells of the interpolation grid for which `wd` is positive (these cells are 'under water' for the given water level) and multiply the number by the cell surface area (`dx ** 2`). To determine the volume, we calculate the volume of the water column and sum the volume for all 'submerged' cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = []\n",
    "V = []\n",
    "for wl in wls:\n",
    "    wd = wl - zi\n",
    "    idx = wd > 0\n",
    "    A.append(np.sum(idx) * dx ** 2)\n",
    "    V.append(np.sum(wd[idx] * dx ** 2))\n",
    "\n",
    "df = pd.DataFrame(index=wls)\n",
    "df['V'] = V\n",
    "df['A'] = A"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the area and volume for each water level calculated, we can fit a polynomial function to the data points so that we'll have a function with which we can convert the water level to a surface area or volume. This function will be used in the mass balance calculations in later sessions. The NumPy function `polyfit` determines the coefficients of the polynomial, which are stored in `p_coef_V` and `p_coef_V`. A polynomial function using these coefficients is easily created using the NumPy function `poly1d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area\n",
    "p_coef_A = np.polyfit(wls, A, 6)\n",
    "p_func_A = np.poly1d(p_coef_A)\n",
    "\n",
    "# Volume\n",
    "p_coef_V = np.polyfit(wls, V, 4)\n",
    "p_func_V = np.poly1d(p_coef_V)\n",
    "\n",
    "# Save the coefficients for later use\n",
    "np.savetxt(f\"p_coef_V_{method}.dat\", p_coef_V)\n",
    "np.savetxt(f\"p_coef_A_{method}.dat\", p_coef_A)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we'll create two graphs to show the data poins and the fitted polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_lines, axs = plt.subplots(ncols=2)\n",
    "\n",
    "# Area\n",
    "ax = axs[0]\n",
    "ax.plot(wls, A, 'C0.')\n",
    "ax.plot(wls, p_func_A(wls), 'C0')\n",
    "ax.set_ylabel(\"Area (m$^2$)\")\n",
    "\n",
    "# Volume\n",
    "ax = axs[1]\n",
    "ax.plot(wls, V, 'C0.')\n",
    "ax.plot(wls, p_func_V(wls), 'C0')\n",
    "ax.set_ylabel(\"Volume (m$^3$)\")\n",
    "\n",
    "# Set some properties for both graphs\n",
    "for ax in axs:\n",
    "    ax.set_xlabel(\"Water level (m)\")\n",
    "    ax.grid(ls=\":\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
